{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "import re\n",
    "from keras.preprocessing import text\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower().strip()\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = gutenberg.sents('carroll-alice.txt') \n",
    "remove_terms = punctuation + '0123456789'\n",
    "alice = [[word.lower() for word in sent if word not in remove_terms] for sent in alice]\n",
    "alice = [' '.join(tok_sent) for tok_sent in alice]\n",
    "alice = list(map(normalize_corpus,alice))\n",
    "alice = [str(sent) for sent in alice if len(str(sent).split()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(alice)\n",
    "word2id = tokenizer.word_index\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in alice]\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    X = []\n",
    "    Y = []\n",
    "    context_length = window_size*2\n",
    "    for words in wids:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):           \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            context = [words[i] for i in range(start, end)if 0 <= i < sentence_length and i != index]\n",
    "            x = sequence.pad_sequences([context], maxlen=context_length)\n",
    "            X.append(x)\n",
    "            Y.append(word)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, inp_size , vocab_size, embedding_dim=100):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 100)\n",
    "        self.activation_function1 = nn.ReLU()        \n",
    "        self.linear2 = nn.Linear(100, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(torch.from_numpy(inputs).long())).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "    \n",
    "model = CBOW(window_size*2,vocab_size)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: tensor(1.0376, grad_fn=<AddBackward0>)\n",
      "Epoch: 2 \tLoss: tensor(0.9594, grad_fn=<AddBackward0>)\n",
      "Epoch: 3 \tLoss: tensor(0.8860, grad_fn=<AddBackward0>)\n",
      "Epoch: 4 \tLoss: tensor(0.8448, grad_fn=<AddBackward0>)\n",
      "Epoch: 5 \tLoss: tensor(0.7961, grad_fn=<AddBackward0>)\n",
      "Epoch: 6 \tLoss: tensor(0.7346, grad_fn=<AddBackward0>)\n",
      "Epoch: 7 \tLoss: tensor(0.6995, grad_fn=<AddBackward0>)\n",
      "Epoch: 8 \tLoss: tensor(0.6336, grad_fn=<AddBackward0>)\n",
      "Epoch: 9 \tLoss: tensor(0.6178, grad_fn=<AddBackward0>)\n",
      "Epoch: 10 \tLoss: tensor(0.5756, grad_fn=<AddBackward0>)\n",
      "Epoch: 11 \tLoss: tensor(0.5612, grad_fn=<AddBackward0>)\n",
      "Epoch: 12 \tLoss: tensor(0.5446, grad_fn=<AddBackward0>)\n",
      "Epoch: 13 \tLoss: tensor(0.5130, grad_fn=<AddBackward0>)\n",
      "Epoch: 14 \tLoss: tensor(0.4864, grad_fn=<AddBackward0>)\n",
      "Epoch: 15 \tLoss: tensor(0.4551, grad_fn=<AddBackward0>)\n",
      "Epoch: 16 \tLoss: tensor(0.4380, grad_fn=<AddBackward0>)\n",
      "Epoch: 17 \tLoss: tensor(0.4262, grad_fn=<AddBackward0>)\n",
      "Epoch: 18 \tLoss: tensor(0.4047, grad_fn=<AddBackward0>)\n",
      "Epoch: 19 \tLoss: tensor(0.3805, grad_fn=<AddBackward0>)\n",
      "Epoch: 20 \tLoss: tensor(0.3759, grad_fn=<AddBackward0>)\n",
      "Epoch: 21 \tLoss: tensor(0.3615, grad_fn=<AddBackward0>)\n",
      "Epoch: 22 \tLoss: tensor(0.3541, grad_fn=<AddBackward0>)\n",
      "Epoch: 23 \tLoss: tensor(0.3477, grad_fn=<AddBackward0>)\n",
      "Epoch: 24 \tLoss: tensor(0.3255, grad_fn=<AddBackward0>)\n",
      "Epoch: 25 \tLoss: tensor(0.3094, grad_fn=<AddBackward0>)\n",
      "Epoch: 26 \tLoss: tensor(0.3105, grad_fn=<AddBackward0>)\n",
      "Epoch: 27 \tLoss: tensor(0.3012, grad_fn=<AddBackward0>)\n",
      "Epoch: 28 \tLoss: tensor(0.2887, grad_fn=<AddBackward0>)\n",
      "Epoch: 29 \tLoss: tensor(0.2787, grad_fn=<AddBackward0>)\n",
      "Epoch: 30 \tLoss: tensor(0.2783, grad_fn=<AddBackward0>)\n",
      "Epoch: 31 \tLoss: tensor(0.2619, grad_fn=<AddBackward0>)\n",
      "Epoch: 32 \tLoss: tensor(0.2605, grad_fn=<AddBackward0>)\n",
      "Epoch: 33 \tLoss: tensor(0.2406, grad_fn=<AddBackward0>)\n",
      "Epoch: 34 \tLoss: tensor(0.2344, grad_fn=<AddBackward0>)\n",
      "Epoch: 35 \tLoss: tensor(0.2312, grad_fn=<AddBackward0>)\n",
      "Epoch: 36 \tLoss: tensor(0.2307, grad_fn=<AddBackward0>)\n",
      "Epoch: 37 \tLoss: tensor(0.2206, grad_fn=<AddBackward0>)\n",
      "Epoch: 38 \tLoss: tensor(0.2250, grad_fn=<AddBackward0>)\n",
      "Epoch: 39 \tLoss: tensor(0.2215, grad_fn=<AddBackward0>)\n",
      "Epoch: 40 \tLoss: tensor(0.2136, grad_fn=<AddBackward0>)\n",
      "Epoch: 41 \tLoss: tensor(0.2058, grad_fn=<AddBackward0>)\n",
      "Epoch: 42 \tLoss: tensor(0.2101, grad_fn=<AddBackward0>)\n",
      "Epoch: 43 \tLoss: tensor(0.2049, grad_fn=<AddBackward0>)\n",
      "Epoch: 44 \tLoss: tensor(0.1997, grad_fn=<AddBackward0>)\n",
      "Epoch: 45 \tLoss: tensor(0.1939, grad_fn=<AddBackward0>)\n",
      "Epoch: 46 \tLoss: tensor(0.2019, grad_fn=<AddBackward0>)\n",
      "Epoch: 47 \tLoss: tensor(0.1913, grad_fn=<AddBackward0>)\n",
      "Epoch: 48 \tLoss: tensor(0.1908, grad_fn=<AddBackward0>)\n",
      "Epoch: 49 \tLoss: tensor(0.1928, grad_fn=<AddBackward0>)\n",
      "Epoch: 50 \tLoss: tensor(0.1862, grad_fn=<AddBackward0>)\n",
      "Epoch: 51 \tLoss: tensor(0.1878, grad_fn=<AddBackward0>)\n",
      "Epoch: 52 \tLoss: tensor(0.1825, grad_fn=<AddBackward0>)\n",
      "Epoch: 53 \tLoss: tensor(0.1870, grad_fn=<AddBackward0>)\n",
      "Epoch: 54 \tLoss: tensor(0.1918, grad_fn=<AddBackward0>)\n",
      "Epoch: 55 \tLoss: tensor(0.1874, grad_fn=<AddBackward0>)\n",
      "Epoch: 56 \tLoss: tensor(0.1941, grad_fn=<AddBackward0>)\n",
      "Epoch: 57 \tLoss: tensor(0.1842, grad_fn=<AddBackward0>)\n",
      "Epoch: 58 \tLoss: tensor(0.1868, grad_fn=<AddBackward0>)\n",
      "Epoch: 59 \tLoss: tensor(0.1846, grad_fn=<AddBackward0>)\n",
      "Epoch: 60 \tLoss: tensor(0.1878, grad_fn=<AddBackward0>)\n",
      "Epoch: 61 \tLoss: tensor(0.1847, grad_fn=<AddBackward0>)\n",
      "Epoch: 62 \tLoss: tensor(0.1869, grad_fn=<AddBackward0>)\n",
      "Epoch: 63 \tLoss: tensor(0.1943, grad_fn=<AddBackward0>)\n",
      "Epoch: 64 \tLoss: tensor(0.1937, grad_fn=<AddBackward0>)\n",
      "Epoch: 65 \tLoss: tensor(0.1891, grad_fn=<AddBackward0>)\n",
      "Epoch: 66 \tLoss: tensor(0.1851, grad_fn=<AddBackward0>)\n",
      "Epoch: 67 \tLoss: tensor(0.1853, grad_fn=<AddBackward0>)\n",
      "Epoch: 68 \tLoss: tensor(0.1854, grad_fn=<AddBackward0>)\n",
      "Epoch: 69 \tLoss: tensor(0.1885, grad_fn=<AddBackward0>)\n",
      "Epoch: 70 \tLoss: tensor(0.1874, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-a128ba8027d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_env/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 100):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    X,Y = generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size)\n",
    "    for x, y in zip(X,Y):\n",
    "        i += 1\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(x[0])\n",
    "        loss = loss_function(log_probs,torch.Tensor([y]).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += loss.data\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>-0.768659</td>\n",
       "      <td>-0.764412</td>\n",
       "      <td>-0.751660</td>\n",
       "      <td>-0.087763</td>\n",
       "      <td>0.488961</td>\n",
       "      <td>-0.163126</td>\n",
       "      <td>0.809996</td>\n",
       "      <td>-0.259608</td>\n",
       "      <td>-1.394410</td>\n",
       "      <td>-0.149467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091195</td>\n",
       "      <td>0.292485</td>\n",
       "      <td>-0.920918</td>\n",
       "      <td>-0.069922</td>\n",
       "      <td>-0.891817</td>\n",
       "      <td>0.103409</td>\n",
       "      <td>0.990430</td>\n",
       "      <td>0.617224</td>\n",
       "      <td>1.631240</td>\n",
       "      <td>-0.547462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alice</th>\n",
       "      <td>-0.969423</td>\n",
       "      <td>0.667060</td>\n",
       "      <td>0.463161</td>\n",
       "      <td>1.194068</td>\n",
       "      <td>-0.437506</td>\n",
       "      <td>0.280481</td>\n",
       "      <td>-0.690799</td>\n",
       "      <td>-0.509485</td>\n",
       "      <td>0.053489</td>\n",
       "      <td>1.955167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716134</td>\n",
       "      <td>0.669029</td>\n",
       "      <td>-1.596573</td>\n",
       "      <td>-0.522355</td>\n",
       "      <td>0.528245</td>\n",
       "      <td>0.727068</td>\n",
       "      <td>-0.724045</td>\n",
       "      <td>1.069976</td>\n",
       "      <td>-0.552863</td>\n",
       "      <td>2.143873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>1.092711</td>\n",
       "      <td>-0.148174</td>\n",
       "      <td>0.208529</td>\n",
       "      <td>-1.195887</td>\n",
       "      <td>-0.461815</td>\n",
       "      <td>0.758844</td>\n",
       "      <td>-0.338614</td>\n",
       "      <td>-0.080432</td>\n",
       "      <td>0.093529</td>\n",
       "      <td>-0.788899</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.243059</td>\n",
       "      <td>-0.081845</td>\n",
       "      <td>-1.045834</td>\n",
       "      <td>1.118203</td>\n",
       "      <td>0.643145</td>\n",
       "      <td>-1.559992</td>\n",
       "      <td>-0.857833</td>\n",
       "      <td>1.104035</td>\n",
       "      <td>-1.704522</td>\n",
       "      <td>-1.247187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>2.062495</td>\n",
       "      <td>-1.024880</td>\n",
       "      <td>-0.702464</td>\n",
       "      <td>-0.464389</td>\n",
       "      <td>0.455664</td>\n",
       "      <td>0.703211</td>\n",
       "      <td>0.194177</td>\n",
       "      <td>-1.719035</td>\n",
       "      <td>-0.850512</td>\n",
       "      <td>-0.154090</td>\n",
       "      <td>...</td>\n",
       "      <td>1.818426</td>\n",
       "      <td>0.360028</td>\n",
       "      <td>0.058361</td>\n",
       "      <td>-1.510473</td>\n",
       "      <td>0.817229</td>\n",
       "      <td>0.461154</td>\n",
       "      <td>1.330694</td>\n",
       "      <td>-0.747295</td>\n",
       "      <td>-2.014755</td>\n",
       "      <td>-0.613017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.166414</td>\n",
       "      <td>-0.027449</td>\n",
       "      <td>-0.113835</td>\n",
       "      <td>0.551269</td>\n",
       "      <td>0.900583</td>\n",
       "      <td>0.436140</td>\n",
       "      <td>-0.227811</td>\n",
       "      <td>1.848397</td>\n",
       "      <td>0.187505</td>\n",
       "      <td>-1.483287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926194</td>\n",
       "      <td>-1.081074</td>\n",
       "      <td>-0.115204</td>\n",
       "      <td>-0.520761</td>\n",
       "      <td>-0.018871</td>\n",
       "      <td>-1.415264</td>\n",
       "      <td>-0.654339</td>\n",
       "      <td>0.672450</td>\n",
       "      <td>0.242389</td>\n",
       "      <td>0.732968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "said   -0.768659 -0.764412 -0.751660 -0.087763  0.488961 -0.163126  0.809996   \n",
       "alice  -0.969423  0.667060  0.463161  1.194068 -0.437506  0.280481 -0.690799   \n",
       "little  1.092711 -0.148174  0.208529 -1.195887 -0.461815  0.758844 -0.338614   \n",
       "one     2.062495 -1.024880 -0.702464 -0.464389  0.455664  0.703211  0.194177   \n",
       "would   0.166414 -0.027449 -0.113835  0.551269  0.900583  0.436140 -0.227811   \n",
       "\n",
       "              7         8         9   ...        90        91        92  \\\n",
       "said   -0.259608 -1.394410 -0.149467  ...  0.091195  0.292485 -0.920918   \n",
       "alice  -0.509485  0.053489  1.955167  ...  0.716134  0.669029 -1.596573   \n",
       "little -0.080432  0.093529 -0.788899  ... -1.243059 -0.081845 -1.045834   \n",
       "one    -1.719035 -0.850512 -0.154090  ...  1.818426  0.360028  0.058361   \n",
       "would   1.848397  0.187505 -1.483287  ... -0.926194 -1.081074 -0.115204   \n",
       "\n",
       "              93        94        95        96        97        98        99  \n",
       "said   -0.069922 -0.891817  0.103409  0.990430  0.617224  1.631240 -0.547462  \n",
       "alice  -0.522355  0.528245  0.727068 -0.724045  1.069976 -0.552863  2.143873  \n",
       "little  1.118203  0.643145 -1.559992 -0.857833  1.104035 -1.704522 -1.247187  \n",
       "one    -1.510473  0.817229  0.461154  1.330694 -0.747295 -2.014755 -0.613017  \n",
       "would  -0.520761 -0.018871 -1.415264 -0.654339  0.672450  0.242389  0.732968  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.embeddings(torch.Tensor([list(range(0,vocab_size))]).long())\n",
    "pd.DataFrame(weights.view(-1,100).tolist(), index=list(id2word.values())[0:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'little': ['farther', 'beginning', 'creep', 'gallons', 'eel'],\n",
       " 'small': ['rock', 'beginning', 'balanced', 'birthday', 'clean'],\n",
       " 'would': ['remarks', 'closed', 'fairy', 'trial', 'wild'],\n",
       " 'child': ['permitted', 'throne', 'apples', 'sheep', 'vegetable'],\n",
       " 'girl': ['busily', 'pressed', 'care', 'drop', 'eggs']}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "weights = weights.view(-1,100)\n",
    "distance_matrix = euclidean_distances(weights.detach().numpy())\n",
    "\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['little', 'small', 'would', 'child','girl']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
